{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-NGram&WSD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaiqEBdCyWz7o+zqXyfr3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliesioo/NLP-NgramAndWSD/blob/master/NLP_NGram%26WSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqCGgRIbqADd",
        "colab_type": "text"
      },
      "source": [
        "**N-Gram Language Modeling(통계적 기반)**\n",
        "* N-gram: n개의 단어가 연속적으로 등장한다\n",
        "* LM: 언어의 시퀀스?를 학습한다\n",
        "\n",
        "\n",
        "통계적 기반\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7atgOkAp7O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4개 단어 연속적으로 나올 가능성\n",
        "# 1의 가능성 + 1나오면 2 나올 가능성 + 1,2나올때 3 나올 사능성....... 다만 너무 작은 값이 나올거라\n",
        "# 아래와 같은 방법으로 계산함\n",
        "\n",
        "p(1,2,3,4) = p(1) * p(2|1) * p(3|1,2) * p(4|1,2,3) # 이 방법으로 하면 로그 취해도 너무 작아  \n",
        "p(1,2,3,4) = p(1) * p(2|1) * p(3|2) * p(4|3) #이거씀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I63eeACs87K",
        "colab_type": "text"
      },
      "source": [
        "**English**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNrs_tRbrklK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# package\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk import ConditionalFreqDist\n",
        "from nltk.probability import ConditionalProbDist, MLEProbDist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62QxejIvtPEm",
        "colab_type": "code",
        "outputId": "46941306-0e76-4a9c-944b-981ee20c1c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# load dataset (corpus 다운받음)\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1JEg-jUtYvj",
        "colab_type": "code",
        "outputId": "f9d3723d-427d-4799-d195-f4161c37d588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# n-gram\n",
        "bigrams = []\n",
        "for tokens in nltk.corpus.brown.sents(): #corpus안에 있는 sentence bigram에 저장\n",
        "  print(tokens) #띄어쓰기 단위\n",
        "\n",
        "  bigrams += ngrams(tokens,2) #bigram = 2 , trigram = 3\n",
        "  print(bigrams) #바이그램 형태로 저장된것 출력\n",
        "  break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
            "[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'), ('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), ('Friday', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('of', \"Atlanta's\"), (\"Atlanta's\", 'recent'), ('recent', 'primary'), ('primary', 'election'), ('election', 'produced'), ('produced', '``'), ('``', 'no'), ('no', 'evidence'), ('evidence', \"''\"), (\"''\", 'that'), ('that', 'any'), ('any', 'irregularities'), ('irregularities', 'took'), ('took', 'place'), ('place', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB0l-nLpt3kR",
        "colab_type": "code",
        "outputId": "5df0a860-73cb-432e-a926-7526b1ae5a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "# n-gram\n",
        "bigrams = []\n",
        "for tokens in nltk.corpus.brown.sents(): #corpus안에 있는 sentence bigram에 저장\n",
        "  bigrams += ngrams(tokens,2) #bigram = 2 , trigram = 3\n",
        "print(bigrams[:10])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'), ('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), ('Friday', 'an'), ('an', 'investigation'), ('investigation', 'of'), ('of', \"Atlanta's\")]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrDmxmfcuO_4",
        "colab_type": "code",
        "outputId": "4cdbe2b0-2518-4be2-d9cc-9a4ab0e74f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#빈도 분포? sentence generation\n",
        "cfd = ConditionalFreqDist(bigrams)\n",
        "cfd['the'].most_common(5) # the 뒤에 가장 많이 나온 단어 5개. 카운트수 나옴. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('same', 597), ('first', 539), ('other', 384), ('most', 376), ('United', 374)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-fDJvHnuylB",
        "colab_type": "code",
        "outputId": "eb8f8978-dfe4-498d-a79b-19132310291e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# sentence prob\n",
        "cpd = ConditionalProbDist(cfd,MLEProbDist)\n",
        "cpd['the'].prob('most') # the 뒤에 most 나온 의 prob. 확률 나옴\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005996332030938522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hLD_6t7up3z",
        "colab_type": "text"
      },
      "source": [
        "**Korean**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbb_aXeMupCN",
        "colab_type": "code",
        "outputId": "2b440c4b-9e97-442e-df2e-c4c71ccf2a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "source": [
        "#package\n",
        "!pip install konlpy\n",
        "import konlpy\n",
        "from konlpy.tag import Kkma"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.3)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/99/7c79cb6801b57fc6dfc7033d9c4c00e0d2892df3b903ff6c269fb76c8442/JPype1-0.7.4-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Installing collected packages: beautifulsoup4, colorama, tweepy, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.4 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSDCcwGTvuZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset (corpus)\n",
        "with konlpy.corpus.kolaw.open('constitution.txt') as f:\n",
        "  corpus = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1xBj3rCv6bT",
        "colab_type": "code",
        "outputId": "299b4846-9e18-4f4f-f696-4ac67d51f044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenization\n",
        "# 띄어쓰기 단위로 나눠지지 않고 형태소로 나눠져야함. 아님 양이 넘 많음\n",
        "kkma = Kkma() #kkma 형태소 분석기. 시간 오래 걸리지만 정확도 높음\n",
        "tokens = kkma.morphs(corpus) \n",
        "print(tokens[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['대한민국', '헌법', '유구', '하', 'ㄴ', '역사', '와', '전통', '에', '빛나']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziQLxfMJwR_Q",
        "colab_type": "code",
        "outputId": "a45f0579-f25c-46b0-972d-3af883fe082c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# n-gram\n",
        "bigrams = ngrams(tokens,2) # bigram에 저장\n",
        "cfd = ConditionalFreqDist(bigrams)\n",
        "cfd['대한민국'].most_common(5) # 대만민국 뒤에 나올 것\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('의', 5), ('은', 3), ('헌법', 1), ('임시', 1), ('영역', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue7MHtoYwqC5",
        "colab_type": "code",
        "outputId": "a40e3bb6-b269-4c6b-d68e-bb05d814f02f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# sentence prob\n",
        "cpd = ConditionalProbDist(cfd, MLEProbDist) # MLEProbDist 11개중 5개\n",
        "cpd['대한민국'].prob('의') # 대만민국 뒤에 의 나올 가능성\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45454545454545453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdFMZwJixBvK",
        "colab_type": "text"
      },
      "source": [
        "**Word-Sense Disambiguation** (중의성 해소)\n",
        "* Lesk alg : 통계적으로 중의성 해소 \n",
        "  * sense: 워드의 대한 사전적 정의 \n",
        "    * 하나 선택해 best-sense로 선출\n",
        "  * max overlap : 최대한 얼마나 겁치는지\n",
        "  * sense가지는 signature,context 얼마나 겁치는지 찾아내고\n",
        "  * overlap > max-overlap: 取代舊max&best-sense\n",
        "  * returen best sense + word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LN0k87iw2CS",
        "colab_type": "code",
        "outputId": "15a331e8-8275-4c37-ea9a-9e7a56fe83dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# packages\n",
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download ('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skaXS8qLylQc",
        "colab_type": "code",
        "outputId": "016f2ea6-8ddd-43b6-c1d6-c3722887d191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# WSD using LESK alg\n",
        "sent = ['It','is','not','because','I','was','trying','to','save','money']\n",
        "lesk(sent,'save','v') # sent안에 등장한 save란 단어 = 5번째 의미다.\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('save.v.05')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-6vnDBhzNod",
        "colab_type": "code",
        "outputId": "fa3ceebb-4e2b-4c2a-8c68-06cae581f534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# 5번째 단어가 무슨 의민지 몰라서 알기 위해\n",
        "for synset in wordnet.synsets('save'):\n",
        "  print(synset,synset.definition())\n",
        "\n",
        "# 5번째 의미: Synset('save.v.05') accumulate money for future use"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('save.n.01') (sports) the act of preventing the opposition from scoring\n",
            "Synset('salvage.v.01') save from ruin, destruction, or harm\n",
            "Synset('save.v.02') to keep up and reserve for personal or special use\n",
            "Synset('save.v.03') bring into safety\n",
            "Synset('save.v.04') spend less; buy at a reduced price\n",
            "Synset('save.v.05') accumulate money for future use\n",
            "Synset('save.v.06') make unnecessary an expenditure or effort\n",
            "Synset('deliver.v.08') save from sins\n",
            "Synset('spare.v.01') refrain from harming\n",
            "Synset('save.v.09') spend sparingly, avoid the waste of\n",
            "Synset('keep_open.v.01') retain rights to\n",
            "Synset('write.v.08') record data on a computer\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}